{"version":3,"sources":["../src/llm/types.ts","../src/llm/client.ts"],"sourcesContent":["import { z } from \"zod\";\n\nexport const LLMProviderSchema = z.enum([\"openai\", \"anthropic\", \"gemini\"]);\nexport type LLMProvider = z.infer<typeof LLMProviderSchema>;\n\nexport const LLMConfigSchema = z.object({\n  provider: LLMProviderSchema,\n  model: z.string().optional(),\n  apiKey: z.string(),\n  temperature: z.number().min(0).max(2).default(0.7),\n  maxTokens: z.number().default(4096),\n});\nexport type LLMConfig = z.infer<typeof LLMConfigSchema>;\n\nexport interface LLMMessage {\n  role: \"system\" | \"user\" | \"assistant\";\n  content: string;\n}\n\nexport interface LLMResponse {\n  content: string;\n  usage?: {\n    promptTokens: number;\n    completionTokens: number;\n    totalTokens: number;\n  };\n  finishReason?: string;\n}\n\nexport interface LLMClient {\n  chat(messages: LLMMessage[]): Promise<LLMResponse>;\n  complete(prompt: string): Promise<string>;\n}\n","import type { LLMClient, LLMConfig, LLMMessage, LLMResponse } from \"./types.js\";\n\ninterface OpenAIResponse {\n  choices: Array<{ message: { content: string }; finish_reason?: string }>;\n  usage?: { prompt_tokens: number; completion_tokens: number; total_tokens: number };\n}\n\ninterface AnthropicResponse {\n  content: Array<{ text: string }>;\n  usage?: { input_tokens: number; output_tokens: number };\n  stop_reason?: string;\n}\n\ninterface GeminiResponse {\n  candidates?: Array<{ content?: { parts?: Array<{ text?: string }> } }>;\n  usageMetadata?: { promptTokenCount?: number; candidatesTokenCount?: number; totalTokenCount?: number };\n}\n\nconst DEFAULT_MODELS: Record<string, string> = {\n  openai: \"gpt-4o\",\n  anthropic: \"claude-sonnet-4-20250514\",\n  gemini: \"gemini-1.5-pro\",\n};\n\nexport class UniversalLLMClient implements LLMClient {\n  private config: LLMConfig;\n  private model: string;\n\n  constructor(config: LLMConfig) {\n    this.config = config;\n    this.model = config.model ?? DEFAULT_MODELS[config.provider];\n  }\n\n  async chat(messages: LLMMessage[]): Promise<LLMResponse> {\n    switch (this.config.provider) {\n      case \"openai\":\n        return this.chatOpenAI(messages);\n      case \"anthropic\":\n        return this.chatAnthropic(messages);\n      case \"gemini\":\n        return this.chatGemini(messages);\n      default:\n        throw new Error(`Unsupported provider: ${this.config.provider}`);\n    }\n  }\n\n  async complete(prompt: string): Promise<string> {\n    const response = await this.chat([{ role: \"user\", content: prompt }]);\n    return response.content;\n  }\n\n  private async chatOpenAI(messages: LLMMessage[]): Promise<LLMResponse> {\n    const response = await fetch(\"https://api.openai.com/v1/chat/completions\", {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\",\n        Authorization: `Bearer ${this.config.apiKey}`,\n      },\n      body: JSON.stringify({\n        model: this.model,\n        messages,\n        temperature: this.config.temperature,\n        max_tokens: this.config.maxTokens,\n      }),\n    });\n\n    if (!response.ok) {\n      throw new Error(`OpenAI API error: ${response.statusText}`);\n    }\n\n    const data = (await response.json()) as OpenAIResponse;\n    return {\n      content: data.choices[0].message.content,\n      usage: {\n        promptTokens: data.usage?.prompt_tokens ?? 0,\n        completionTokens: data.usage?.completion_tokens ?? 0,\n        totalTokens: data.usage?.total_tokens ?? 0,\n      },\n      finishReason: data.choices[0].finish_reason,\n    };\n  }\n\n  private async chatAnthropic(messages: LLMMessage[]): Promise<LLMResponse> {\n    const systemMessage = messages.find((m) => m.role === \"system\");\n    const userMessages = messages.filter((m) => m.role !== \"system\");\n\n    const response = await fetch(\"https://api.anthropic.com/v1/messages\", {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\",\n        \"x-api-key\": this.config.apiKey,\n        \"anthropic-version\": \"2023-06-01\",\n      },\n      body: JSON.stringify({\n        model: this.model,\n        max_tokens: this.config.maxTokens,\n        system: systemMessage?.content,\n        messages: userMessages.map((m) => ({\n          role: m.role === \"assistant\" ? \"assistant\" : \"user\",\n          content: m.content,\n        })),\n      }),\n    });\n\n    if (!response.ok) {\n      throw new Error(`Anthropic API error: ${response.statusText}`);\n    }\n\n    const data = (await response.json()) as AnthropicResponse;\n    return {\n      content: data.content[0].text,\n      usage: {\n        promptTokens: data.usage?.input_tokens ?? 0,\n        completionTokens: data.usage?.output_tokens ?? 0,\n        totalTokens:\n          (data.usage?.input_tokens ?? 0) + (data.usage?.output_tokens ?? 0),\n      },\n      finishReason: data.stop_reason,\n    };\n  }\n\n  private async chatGemini(messages: LLMMessage[]): Promise<LLMResponse> {\n    const systemMessage = messages.find((m) => m.role === \"system\");\n    const contents = messages\n      .filter((m) => m.role !== \"system\")\n      .map((m) => ({\n        role: m.role === \"assistant\" ? \"model\" : \"user\",\n        parts: [{ text: m.content }],\n      }));\n\n    const url = `https://generativelanguage.googleapis.com/v1beta/models/${this.model}:generateContent?key=${this.config.apiKey}`;\n\n    const response = await fetch(url, {\n      method: \"POST\",\n      headers: { \"Content-Type\": \"application/json\" },\n      body: JSON.stringify({\n        contents,\n        systemInstruction: systemMessage\n          ? { parts: [{ text: systemMessage.content }] }\n          : undefined,\n        generationConfig: {\n          temperature: this.config.temperature,\n          maxOutputTokens: this.config.maxTokens,\n        },\n      }),\n    });\n\n    if (!response.ok) {\n      throw new Error(`Gemini API error: ${response.statusText}`);\n    }\n\n    const data = (await response.json()) as GeminiResponse;\n    return {\n      content: data.candidates?.[0]?.content?.parts?.[0]?.text ?? \"\",\n      usage: {\n        promptTokens: data.usageMetadata?.promptTokenCount ?? 0,\n        completionTokens: data.usageMetadata?.candidatesTokenCount ?? 0,\n        totalTokens: data.usageMetadata?.totalTokenCount ?? 0,\n      },\n    };\n  }\n}\n\nexport function createLLMClient(config: LLMConfig): LLMClient {\n  return new UniversalLLMClient(config);\n}\n"],"mappings":";AAAA,SAAS,SAAS;AAEX,IAAM,oBAAoB,EAAE,KAAK,CAAC,UAAU,aAAa,QAAQ,CAAC;AAGlE,IAAM,kBAAkB,EAAE,OAAO;AAAA,EACtC,UAAU;AAAA,EACV,OAAO,EAAE,OAAO,EAAE,SAAS;AAAA,EAC3B,QAAQ,EAAE,OAAO;AAAA,EACjB,aAAa,EAAE,OAAO,EAAE,IAAI,CAAC,EAAE,IAAI,CAAC,EAAE,QAAQ,GAAG;AAAA,EACjD,WAAW,EAAE,OAAO,EAAE,QAAQ,IAAI;AACpC,CAAC;;;ACOD,IAAM,iBAAyC;AAAA,EAC7C,QAAQ;AAAA,EACR,WAAW;AAAA,EACX,QAAQ;AACV;AAEO,IAAM,qBAAN,MAA8C;AAAA,EAC3C;AAAA,EACA;AAAA,EAER,YAAY,QAAmB;AAC7B,SAAK,SAAS;AACd,SAAK,QAAQ,OAAO,SAAS,eAAe,OAAO,QAAQ;AAAA,EAC7D;AAAA,EAEA,MAAM,KAAK,UAA8C;AACvD,YAAQ,KAAK,OAAO,UAAU;AAAA,MAC5B,KAAK;AACH,eAAO,KAAK,WAAW,QAAQ;AAAA,MACjC,KAAK;AACH,eAAO,KAAK,cAAc,QAAQ;AAAA,MACpC,KAAK;AACH,eAAO,KAAK,WAAW,QAAQ;AAAA,MACjC;AACE,cAAM,IAAI,MAAM,yBAAyB,KAAK,OAAO,QAAQ,EAAE;AAAA,IACnE;AAAA,EACF;AAAA,EAEA,MAAM,SAAS,QAAiC;AAC9C,UAAM,WAAW,MAAM,KAAK,KAAK,CAAC,EAAE,MAAM,QAAQ,SAAS,OAAO,CAAC,CAAC;AACpE,WAAO,SAAS;AAAA,EAClB;AAAA,EAEA,MAAc,WAAW,UAA8C;AACrE,UAAM,WAAW,MAAM,MAAM,8CAA8C;AAAA,MACzE,QAAQ;AAAA,MACR,SAAS;AAAA,QACP,gBAAgB;AAAA,QAChB,eAAe,UAAU,KAAK,OAAO,MAAM;AAAA,MAC7C;AAAA,MACA,MAAM,KAAK,UAAU;AAAA,QACnB,OAAO,KAAK;AAAA,QACZ;AAAA,QACA,aAAa,KAAK,OAAO;AAAA,QACzB,YAAY,KAAK,OAAO;AAAA,MAC1B,CAAC;AAAA,IACH,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,IAAI,MAAM,qBAAqB,SAAS,UAAU,EAAE;AAAA,IAC5D;AAEA,UAAM,OAAQ,MAAM,SAAS,KAAK;AAClC,WAAO;AAAA,MACL,SAAS,KAAK,QAAQ,CAAC,EAAE,QAAQ;AAAA,MACjC,OAAO;AAAA,QACL,cAAc,KAAK,OAAO,iBAAiB;AAAA,QAC3C,kBAAkB,KAAK,OAAO,qBAAqB;AAAA,QACnD,aAAa,KAAK,OAAO,gBAAgB;AAAA,MAC3C;AAAA,MACA,cAAc,KAAK,QAAQ,CAAC,EAAE;AAAA,IAChC;AAAA,EACF;AAAA,EAEA,MAAc,cAAc,UAA8C;AACxE,UAAM,gBAAgB,SAAS,KAAK,CAAC,MAAM,EAAE,SAAS,QAAQ;AAC9D,UAAM,eAAe,SAAS,OAAO,CAAC,MAAM,EAAE,SAAS,QAAQ;AAE/D,UAAM,WAAW,MAAM,MAAM,yCAAyC;AAAA,MACpE,QAAQ;AAAA,MACR,SAAS;AAAA,QACP,gBAAgB;AAAA,QAChB,aAAa,KAAK,OAAO;AAAA,QACzB,qBAAqB;AAAA,MACvB;AAAA,MACA,MAAM,KAAK,UAAU;AAAA,QACnB,OAAO,KAAK;AAAA,QACZ,YAAY,KAAK,OAAO;AAAA,QACxB,QAAQ,eAAe;AAAA,QACvB,UAAU,aAAa,IAAI,CAAC,OAAO;AAAA,UACjC,MAAM,EAAE,SAAS,cAAc,cAAc;AAAA,UAC7C,SAAS,EAAE;AAAA,QACb,EAAE;AAAA,MACJ,CAAC;AAAA,IACH,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,IAAI,MAAM,wBAAwB,SAAS,UAAU,EAAE;AAAA,IAC/D;AAEA,UAAM,OAAQ,MAAM,SAAS,KAAK;AAClC,WAAO;AAAA,MACL,SAAS,KAAK,QAAQ,CAAC,EAAE;AAAA,MACzB,OAAO;AAAA,QACL,cAAc,KAAK,OAAO,gBAAgB;AAAA,QAC1C,kBAAkB,KAAK,OAAO,iBAAiB;AAAA,QAC/C,cACG,KAAK,OAAO,gBAAgB,MAAM,KAAK,OAAO,iBAAiB;AAAA,MACpE;AAAA,MACA,cAAc,KAAK;AAAA,IACrB;AAAA,EACF;AAAA,EAEA,MAAc,WAAW,UAA8C;AACrE,UAAM,gBAAgB,SAAS,KAAK,CAAC,MAAM,EAAE,SAAS,QAAQ;AAC9D,UAAM,WAAW,SACd,OAAO,CAAC,MAAM,EAAE,SAAS,QAAQ,EACjC,IAAI,CAAC,OAAO;AAAA,MACX,MAAM,EAAE,SAAS,cAAc,UAAU;AAAA,MACzC,OAAO,CAAC,EAAE,MAAM,EAAE,QAAQ,CAAC;AAAA,IAC7B,EAAE;AAEJ,UAAM,MAAM,2DAA2D,KAAK,KAAK,wBAAwB,KAAK,OAAO,MAAM;AAE3H,UAAM,WAAW,MAAM,MAAM,KAAK;AAAA,MAChC,QAAQ;AAAA,MACR,SAAS,EAAE,gBAAgB,mBAAmB;AAAA,MAC9C,MAAM,KAAK,UAAU;AAAA,QACnB;AAAA,QACA,mBAAmB,gBACf,EAAE,OAAO,CAAC,EAAE,MAAM,cAAc,QAAQ,CAAC,EAAE,IAC3C;AAAA,QACJ,kBAAkB;AAAA,UAChB,aAAa,KAAK,OAAO;AAAA,UACzB,iBAAiB,KAAK,OAAO;AAAA,QAC/B;AAAA,MACF,CAAC;AAAA,IACH,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,IAAI,MAAM,qBAAqB,SAAS,UAAU,EAAE;AAAA,IAC5D;AAEA,UAAM,OAAQ,MAAM,SAAS,KAAK;AAClC,WAAO;AAAA,MACL,SAAS,KAAK,aAAa,CAAC,GAAG,SAAS,QAAQ,CAAC,GAAG,QAAQ;AAAA,MAC5D,OAAO;AAAA,QACL,cAAc,KAAK,eAAe,oBAAoB;AAAA,QACtD,kBAAkB,KAAK,eAAe,wBAAwB;AAAA,QAC9D,aAAa,KAAK,eAAe,mBAAmB;AAAA,MACtD;AAAA,IACF;AAAA,EACF;AACF;AAEO,SAAS,gBAAgB,QAA8B;AAC5D,SAAO,IAAI,mBAAmB,MAAM;AACtC;","names":[]}
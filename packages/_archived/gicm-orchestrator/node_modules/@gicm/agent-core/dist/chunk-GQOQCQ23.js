// src/llm/types.ts
import { z } from "zod";
var LLMProviderSchema = z.enum(["openai", "anthropic", "gemini"]);
var EffortLevelSchema = z.enum(["low", "medium", "high"]);
var ExtendedThinkingSchema = z.object({
  enabled: z.boolean().default(false),
  budgetTokens: z.number().min(1e3).max(128e3).default(8e3)
});
var LLMConfigSchema = z.object({
  provider: LLMProviderSchema,
  model: z.string().optional(),
  apiKey: z.string(),
  temperature: z.number().min(0).max(2).default(0.7),
  maxTokens: z.number().default(4096),
  // Opus 4.5 features
  effort: EffortLevelSchema.optional(),
  extendedThinking: ExtendedThinkingSchema.optional(),
  // Prompt caching (90% savings)
  promptCaching: z.boolean().optional()
});

// src/llm/client.ts
var DEFAULT_MODELS = {
  openai: "gpt-4o",
  anthropic: "claude-opus-4-5-20251101",
  // Updated to Opus 4.5
  gemini: "gemini-2.0-flash"
};
var EFFORT_MAX_TOKENS = {
  low: 2048,
  medium: 8192,
  high: 32e3
};
var UniversalLLMClient = class {
  config;
  model;
  constructor(config) {
    this.config = config;
    this.model = config.model ?? DEFAULT_MODELS[config.provider];
  }
  getConfig() {
    return { ...this.config };
  }
  async chat(messages) {
    const startTime = Date.now();
    let response;
    switch (this.config.provider) {
      case "openai":
        response = await this.chatOpenAI(messages);
        break;
      case "anthropic":
        response = await this.chatAnthropic(messages);
        break;
      case "gemini":
        response = await this.chatGemini(messages);
        break;
      default:
        throw new Error(`Unsupported provider: ${this.config.provider}`);
    }
    response.latencyMs = Date.now() - startTime;
    return response;
  }
  async complete(prompt) {
    const response = await this.chat([{ role: "user", content: prompt }]);
    return response.content;
  }
  // ===========================================================================
  // OPENAI
  // ===========================================================================
  async chatOpenAI(messages) {
    const maxTokens = this.config.effort ? EFFORT_MAX_TOKENS[this.config.effort] : this.config.maxTokens;
    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${this.config.apiKey}`
      },
      body: JSON.stringify({
        model: this.model,
        messages,
        temperature: this.config.temperature,
        max_tokens: maxTokens
      })
    });
    if (!response.ok) {
      const error = await response.text();
      throw new Error(`OpenAI API error: ${response.status} - ${error}`);
    }
    const data = await response.json();
    return {
      content: data.choices[0].message.content,
      usage: {
        promptTokens: data.usage?.prompt_tokens ?? 0,
        completionTokens: data.usage?.completion_tokens ?? 0,
        totalTokens: data.usage?.total_tokens ?? 0
      },
      finishReason: data.choices[0].finish_reason,
      model: data.model
    };
  }
  // ===========================================================================
  // ANTHROPIC (with Opus 4.5 features)
  // ===========================================================================
  async chatAnthropic(messages) {
    const systemMessage = messages.find((m) => m.role === "system");
    const userMessages = messages.filter((m) => m.role !== "system");
    const body = {
      model: this.model,
      max_tokens: this.config.maxTokens,
      messages: userMessages.map((m) => ({
        role: m.role === "assistant" ? "assistant" : "user",
        content: m.content
      }))
    };
    if (systemMessage) {
      if (this.config.promptCaching) {
        body.system = [
          {
            type: "text",
            text: systemMessage.content,
            cache_control: { type: "ephemeral" }
          }
        ];
      } else {
        body.system = systemMessage.content;
      }
    }
    if (this.config.extendedThinking?.enabled) {
      body.thinking = {
        type: "enabled",
        budget_tokens: this.config.extendedThinking.budgetTokens
      };
      body.temperature = 1;
    } else {
      body.temperature = this.config.temperature;
    }
    const headers = {
      "Content-Type": "application/json",
      "x-api-key": this.config.apiKey,
      "anthropic-version": "2023-06-01"
    };
    if (this.config.promptCaching) {
      headers["anthropic-beta"] = "prompt-caching-2024-07-31";
    }
    if (this.config.extendedThinking?.enabled) {
      const existingBeta = headers["anthropic-beta"];
      headers["anthropic-beta"] = existingBeta ? `${existingBeta},interleaved-thinking-2025-05-14` : "interleaved-thinking-2025-05-14";
    }
    const response = await fetch("https://api.anthropic.com/v1/messages", {
      method: "POST",
      headers,
      body: JSON.stringify(body)
    });
    if (!response.ok) {
      const error = await response.text();
      throw new Error(`Anthropic API error: ${response.status} - ${error}`);
    }
    const data = await response.json();
    let content = "";
    let thinking = "";
    for (const block of data.content) {
      if (block.type === "text" && block.text) {
        content += block.text;
      } else if (block.type === "thinking" && block.thinking) {
        thinking += block.thinking;
      }
    }
    const inputTokens = data.usage?.input_tokens ?? 0;
    const outputTokens = data.usage?.output_tokens ?? 0;
    const cachedTokens = (data.usage?.cache_read_input_tokens ?? 0) + (data.usage?.cache_creation_input_tokens ?? 0);
    return {
      content,
      thinking: thinking || void 0,
      usage: {
        promptTokens: inputTokens,
        completionTokens: outputTokens,
        cachedTokens: cachedTokens || void 0,
        totalTokens: inputTokens + outputTokens
      },
      finishReason: data.stop_reason,
      model: data.model
    };
  }
  // ===========================================================================
  // GEMINI
  // ===========================================================================
  async chatGemini(messages) {
    const systemMessage = messages.find((m) => m.role === "system");
    const contents = messages.filter((m) => m.role !== "system").map((m) => ({
      role: m.role === "assistant" ? "model" : "user",
      parts: [{ text: m.content }]
    }));
    const maxTokens = this.config.effort ? EFFORT_MAX_TOKENS[this.config.effort] : this.config.maxTokens;
    const url = `https://generativelanguage.googleapis.com/v1beta/models/${this.model}:generateContent?key=${this.config.apiKey}`;
    const response = await fetch(url, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        contents,
        systemInstruction: systemMessage ? { parts: [{ text: systemMessage.content }] } : void 0,
        generationConfig: {
          temperature: this.config.temperature,
          maxOutputTokens: maxTokens
        }
      })
    });
    if (!response.ok) {
      const error = await response.text();
      throw new Error(`Gemini API error: ${response.status} - ${error}`);
    }
    const data = await response.json();
    return {
      content: data.candidates?.[0]?.content?.parts?.[0]?.text ?? "",
      usage: {
        promptTokens: data.usageMetadata?.promptTokenCount ?? 0,
        completionTokens: data.usageMetadata?.candidatesTokenCount ?? 0,
        totalTokens: data.usageMetadata?.totalTokenCount ?? 0
      },
      model: this.model
    };
  }
};
function createLLMClient(config) {
  return new UniversalLLMClient(config);
}
function createTurboClient(apiKey) {
  return new UniversalLLMClient({
    provider: "anthropic",
    model: "claude-haiku-3-5-20241022",
    apiKey,
    temperature: 0.3,
    maxTokens: 2048,
    effort: "low"
  });
}
function createPowerClient(apiKey) {
  return new UniversalLLMClient({
    provider: "anthropic",
    model: "claude-opus-4-5-20251101",
    apiKey,
    temperature: 0.5,
    maxTokens: 16384,
    effort: "high",
    extendedThinking: {
      enabled: true,
      budgetTokens: 16e3
    },
    promptCaching: true
  });
}
function createBalancedClient(apiKey) {
  return new UniversalLLMClient({
    provider: "anthropic",
    model: "claude-sonnet-4-20250514",
    apiKey,
    temperature: 0.7,
    maxTokens: 8192,
    effort: "medium",
    promptCaching: true
  });
}

export {
  LLMProviderSchema,
  EffortLevelSchema,
  ExtendedThinkingSchema,
  LLMConfigSchema,
  UniversalLLMClient,
  createLLMClient,
  createTurboClient,
  createPowerClient,
  createBalancedClient
};
//# sourceMappingURL=chunk-GQOQCQ23.js.map